{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# IND320 Project Part 4: Machine Learning & Advanced Analytics\n\n**Student:** Hasan Elahi  \n**Course:** IND320 - Data to Decision  \n**Institution:** NMBU\n\n**GitHub Repository:** https://github.com/hasanelahi7/hasanelahi7-ind320-dashboard  \n**Streamlit App:** https://hasanelahi7-ind320-dashboard-final.streamlit.app/"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Imports](#setup)\n",
    "2. [Spark and Cassandra Configuration](#spark-setup)\n",
    "3. [Data Collection: Production 2022-2024](#production-data)\n",
    "4. [Data Collection: Consumption 2021-2024](#consumption-data)\n",
    "5. [Store in Cassandra](#cassandra-storage)\n",
    "6. [Upload to MongoDB](#mongodb-upload)\n",
    "7. [Data Verification and Statistics](#verification)\n",
    "8. [Project Work Log](#work-log)\n",
    "9. [AI Usage Documentation](#ai-usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## 1. Setup and Imports\n",
    "\n",
    "Install and import necessary libraries for data fetching, processing, and storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "# !pip install pyspark pymongo requests pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 All imports successful\n",
      "Python version: 3.8.16 (default, Mar  1 2023, 21:19:10) \n",
      "[Clang 14.0.6 ]\n",
      "Pandas version: 1.5.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from pymongo import MongoClient, ASCENDING\n",
    "from pymongo.errors import BulkWriteError\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"\u2705 All imports successful\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='spark-setup'></a>\n",
    "## 2. Spark and Cassandra Configuration\n",
    "\n",
    "Configure Java environment and initialize Spark session with Cassandra connector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME = /Library/Java/JavaVirtualMachines/temurin-11.jdk/Contents/Home\n",
      "Java version:\n",
      "openjdk version \"11.0.28\" 2025-07-15\n",
      "OpenJDK Runtime Environment Temurin-11.0.28+6 (build 11.0.28+6)\n",
      "OpenJDK 64-Bit Server VM Temurin-11.0.28+6 (build 11.0.28+6, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "# Set Java Home for Spark\n",
    "JAVA11 = \"/Library/Java/JavaVirtualMachines/temurin-11.jdk/Contents/Home\"\n",
    "os.environ[\"JAVA_HOME\"] = JAVA11\n",
    "os.environ[\"PATH\"] = JAVA11 + \"/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "print(\"JAVA_HOME =\", os.environ.get(\"JAVA_HOME\"))\n",
    "\n",
    "# Verify Java version\n",
    "import subprocess\n",
    "java_version = subprocess.run([\"java\", \"-version\"], capture_output=True, text=True).stderr.strip()\n",
    "print(\"Java version:\")\n",
    "print(java_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Spark Python to: /Users/hasanelahi/miniconda3/bin/python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/20 23:53:26 WARN Utils: Your hostname, Hasans-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.11.148 instead (on interface en0)\n",
      "25/11/20 23:53:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/hasanelahi/miniconda3/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/hasanelahi/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/hasanelahi/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-10f6ae28-1358-47a8-afd2-d90518409e79;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.5.1 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.1 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 in central\n",
      "\tfound org.apache.cassandra#java-driver-core-shaded;4.18.1 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.1 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound org.apache.cassandra#java-driver-mapper-runtime;4.18.1 in central\n",
      "\tfound org.apache.cassandra#java-driver-query-builder;4.18.1 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.19 in central\n",
      ":: resolution report :: resolve 1043ms :: artifacts dl 41ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.1 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.1 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.5.1 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.cassandra#java-driver-core-shaded;4.18.1 from central in [default]\n",
      "\torg.apache.cassandra#java-driver-mapper-runtime;4.18.1 from central in [default]\n",
      "\torg.apache.cassandra#java-driver-query-builder;4.18.1 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.19 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.11.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   16  |   0   |   0   |   0   ||   16  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-10f6ae28-1358-47a8-afd2-d90518409e79\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 16 already retrieved (0kB/22ms)\n",
      "25/11/20 23:53:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Spark session created successfully\n",
      "Spark version: 3.5.7\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Fix Python version mismatch: tell Spark workers to use the same Python as the driver\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "print(f\"Setting Spark Python to: {sys.executable}\")\n",
    "\n",
    "# Create Spark session with Cassandra connector\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"IND320-Part4\")\n",
    "    .config(\"spark.master\", \"local[*]\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.1\")\n",
    "    .config(\"spark.cassandra.connection.host\", \"127.0.0.1\")\n",
    "    .config(\"spark.cassandra.connection.port\", \"9042\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"\u2705 Spark session created successfully\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='production-data'></a>\n",
    "## 3. Data Collection: Production 2022-2024\n",
    "\n",
    "Fetch hourly production data for all Norwegian price areas (NO1-NO5) for years 2022, 2023, and 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Helper function defined\n"
     ]
    }
   ],
   "source": [
    "# Helper function to generate month ranges\n",
    "def month_ranges(year: int):\n",
    "    \"\"\"Generate start and end timestamps for each month of a given year.\"\"\"\n",
    "    tz = dt.timezone.utc\n",
    "    for m in range(1, 13):\n",
    "        start = dt.datetime(year, m, 1, 0, 0, tzinfo=tz)\n",
    "        if m == 12:\n",
    "            end = dt.datetime(year+1, 1, 1, 0, 0, tzinfo=tz) - dt.timedelta(hours=1)\n",
    "        else:\n",
    "            end = dt.datetime(year, m+1, 1, 0, 0, tzinfo=tz) - dt.timedelta(hours=1)\n",
    "        yield m, start.isoformat().replace(\"+00:00\", \"Z\"), end.isoformat().replace(\"+00:00\", \"Z\")\n",
    "\n",
    "print(\"\u2705 Helper function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Production fetch function defined\n"
     ]
    }
   ],
   "source": [
    "def fetch_production_month(start_iso: str, end_iso: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch hourly production per group for ALL price areas between start_iso and end_iso.\n",
    "    Uses /price-areas endpoint with PRODUCTION_PER_GROUP_MBA_HOUR dataset.\n",
    "    \"\"\"\n",
    "    BASE = \"https://api.elhub.no/energy-data/v0\"\n",
    "    url = f\"{BASE}/price-areas\"\n",
    "    params = {\n",
    "        \"dataset\": \"PRODUCTION_PER_GROUP_MBA_HOUR\",\n",
    "        \"startTime\": start_iso,\n",
    "        \"endTime\": end_iso,\n",
    "        \"pageSize\": 200\n",
    "    }\n",
    "\n",
    "    all_rows = []\n",
    "\n",
    "    def extract_page(u, p):\n",
    "        r = requests.get(u, params=p, headers={\"Accept\": \"application/json\"}, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        js = r.json()\n",
    "        data = js.get(\"data\", [])\n",
    "        for item in data:\n",
    "            attr = item.get(\"attributes\", {}) or {}\n",
    "            area_name = attr.get(\"name\")  # e.g., \"NO1\"\n",
    "            rows = attr.get(\"productionPerGroupMbaHour\", []) or []\n",
    "            for row in rows:\n",
    "                row = dict(row)\n",
    "                if \"priceArea\" not in row or not row[\"priceArea\"]:\n",
    "                    row[\"priceArea\"] = area_name\n",
    "                all_rows.append(row)\n",
    "        next_url = js.get(\"links\", {}).get(\"next\")\n",
    "        return next_url\n",
    "\n",
    "    next_url = url\n",
    "    next_params = params\n",
    "    while next_url:\n",
    "        next_url = extract_page(next_url, next_params)\n",
    "        next_params = None\n",
    "\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    df = df.rename(columns={\n",
    "        \"priceArea\": \"priceArea\",\n",
    "        \"productionGroup\": \"productionGroup\",\n",
    "        \"startTime\": \"startTime\",\n",
    "        \"quantityKwh\": \"quantityKwh\"\n",
    "    })\n",
    "    keep = [c for c in [\"priceArea\", \"productionGroup\", \"startTime\", \"quantityKwh\"] if c in df.columns]\n",
    "    df = df[keep].copy()\n",
    "    df[\"startTime\"] = pd.to_datetime(df[\"startTime\"], utc=True, errors=\"coerce\")\n",
    "    df[\"quantityKwh\"] = pd.to_numeric(df[\"quantityKwh\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"priceArea\", \"productionGroup\", \"startTime\"])\n",
    "    return df\n",
    "\n",
    "print(\"\u2705 Production fetch function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching production data for 2022-2024...\n",
      "This will take several minutes. Please be patient.\n",
      "\n",
      "\n",
      "=== Year 2022 ===\n",
      "  Month 01: 18,818 rows\n",
      "  Month 02: 18,818 rows\n",
      "  Month 03: 18,818 rows\n",
      "  Month 04: 18,818 rows\n",
      "  Month 05: 18,818 rows\n",
      "  Month 06: 18,818 rows\n",
      "  Month 07: 18,818 rows\n",
      "  Month 08: 18,818 rows\n",
      "  Month 09: 18,818 rows\n",
      "  Month 10: 18,818 rows\n",
      "  Month 11: 18,818 rows\n",
      "  Month 12: 18,818 rows\n",
      "Year 2022 total: 225,816 rows\n",
      "\n",
      "=== Year 2023 ===\n",
      "  Month 01: 18,818 rows\n",
      "  Month 02: 18,818 rows\n",
      "  Month 03: 18,818 rows\n",
      "  Month 04: 18,818 rows\n",
      "  Month 05: 18,818 rows\n",
      "  Month 06: 18,818 rows\n",
      "  Month 07: 18,818 rows\n",
      "  Month 08: 18,818 rows\n",
      "  Month 09: 18,818 rows\n",
      "  Month 10: 18,818 rows\n",
      "  Month 11: 18,818 rows\n",
      "  Month 12: 18,818 rows\n",
      "Year 2023 total: 225,816 rows\n",
      "\n",
      "=== Year 2024 ===\n",
      "  Month 01: 18,818 rows\n",
      "  Month 02: 18,818 rows\n",
      "  Month 03: 18,818 rows\n",
      "  Month 04: 18,818 rows\n",
      "  Month 05: 18,818 rows\n",
      "  Month 06: 18,818 rows\n",
      "  Month 07: 18,818 rows\n",
      "  Month 08: 18,818 rows\n",
      "  Month 09: 18,818 rows\n",
      "  Month 10: 18,818 rows\n",
      "  Month 11: 18,818 rows\n",
      "  Month 12: 18,818 rows\n",
      "Year 2024 total: 225,816 rows\n",
      "\n",
      "\u2705 Production data 2022-2024 fetched: 677,448 total rows\n",
      "Date range: 2025-10-20 22:00:00+00:00 to 2025-11-19 22:00:00+00:00\n",
      "\n",
      "Sample data:\n",
      "  priceArea productionGroup                 startTime  quantityKwh\n",
      "0       NO1           hydro 2025-10-20 22:00:00+00:00    1970293.2\n",
      "1       NO1           hydro 2025-10-20 23:00:00+00:00    1882895.2\n",
      "2       NO1           hydro 2025-10-21 00:00:00+00:00    1810273.5\n",
      "3       NO1           hydro 2025-10-21 01:00:00+00:00    1779983.2\n",
      "4       NO1           hydro 2025-10-21 02:00:00+00:00    1777715.4\n"
     ]
    }
   ],
   "source": [
    "# Fetch production data for 2022-2024\n",
    "production_years = [2022, 2023, 2024]\n",
    "production_parts = []\n",
    "\n",
    "print(\"Fetching production data for 2022-2024...\")\n",
    "print(\"This will take several minutes. Please be patient.\\n\")\n",
    "\n",
    "for year in production_years:\n",
    "    print(f\"\\n=== Year {year} ===\")\n",
    "    year_parts = []\n",
    "    for m, s, e in month_ranges(year):\n",
    "        try:\n",
    "            df_m = fetch_production_month(s, e)\n",
    "            print(f\"  Month {m:02d}: {len(df_m):,} rows\")\n",
    "            year_parts.append(df_m)\n",
    "        except Exception as ex:\n",
    "            print(f\"  Month {m:02d}: ERROR - {ex}\")\n",
    "    \n",
    "    if year_parts:\n",
    "        year_df = pd.concat(year_parts, ignore_index=True)\n",
    "        production_parts.append(year_df)\n",
    "        print(f\"Year {year} total: {len(year_df):,} rows\")\n",
    "\n",
    "# Combine all years\n",
    "if production_parts:\n",
    "    production_2022_2024 = pd.concat(production_parts, ignore_index=True)\n",
    "    print(f\"\\n\u2705 Production data 2022-2024 fetched: {len(production_2022_2024):,} total rows\")\n",
    "    print(f\"Date range: {production_2022_2024['startTime'].min()} to {production_2022_2024['startTime'].max()}\")\n",
    "    print(f\"\\nSample data:\")\n",
    "    print(production_2022_2024.head())\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f No production data fetched\")\n",
    "    production_2022_2024 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='consumption-data'></a>\n",
    "## 4. Data Collection: Consumption 2021-2024\n",
    "\n",
    "Fetch hourly consumption data for all Norwegian price areas for years 2021, 2022, 2023, and 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Consumption fetch function defined\n"
     ]
    }
   ],
   "source": [
    "def fetch_consumption_month(start_iso: str, end_iso: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch hourly consumption per group for ALL price areas.\n",
    "    Uses /price-areas endpoint with CONSUMPTION_PER_GROUP_MBA_HOUR dataset.\n",
    "    \"\"\"\n",
    "    BASE = \"https://api.elhub.no/energy-data/v0\"\n",
    "    url = f\"{BASE}/price-areas\"\n",
    "    params = {\n",
    "        \"dataset\": \"CONSUMPTION_PER_GROUP_MBA_HOUR\",\n",
    "        \"startTime\": start_iso,\n",
    "        \"endTime\": end_iso,\n",
    "        \"pageSize\": 200\n",
    "    }\n",
    "\n",
    "    all_rows = []\n",
    "\n",
    "    def extract_page(u, p):\n",
    "        r = requests.get(u, params=p, headers={\"Accept\": \"application/json\"}, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        js = r.json()\n",
    "        data = js.get(\"data\", [])\n",
    "        for item in data:\n",
    "            attr = item.get(\"attributes\", {}) or {}\n",
    "            area_name = attr.get(\"name\")\n",
    "            rows = attr.get(\"consumptionPerGroupMbaHour\", []) or []\n",
    "            for row in rows:\n",
    "                row = dict(row)\n",
    "                if \"priceArea\" not in row or not row[\"priceArea\"]:\n",
    "                    row[\"priceArea\"] = area_name\n",
    "                all_rows.append(row)\n",
    "        next_url = js.get(\"links\", {}).get(\"next\")\n",
    "        return next_url\n",
    "\n",
    "    next_url = url\n",
    "    next_params = params\n",
    "    while next_url:\n",
    "        next_url = extract_page(next_url, next_params)\n",
    "        next_params = None\n",
    "\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    df = df.rename(columns={\n",
    "        \"priceArea\": \"priceArea\",\n",
    "        \"consumptionGroup\": \"consumptionGroup\",\n",
    "        \"startTime\": \"startTime\",\n",
    "        \"quantityKwh\": \"quantityKwh\"\n",
    "    })\n",
    "    keep = [c for c in [\"priceArea\", \"consumptionGroup\", \"startTime\", \"quantityKwh\"] if c in df.columns]\n",
    "    df = df[keep].copy()\n",
    "    df[\"startTime\"] = pd.to_datetime(df[\"startTime\"], utc=True, errors=\"coerce\")\n",
    "    df[\"quantityKwh\"] = pd.to_numeric(df[\"quantityKwh\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"priceArea\", \"consumptionGroup\", \"startTime\"])\n",
    "    return df\n",
    "\n",
    "print(\"\u2705 Consumption fetch function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching consumption data for 2021-2024...\n",
      "This will take several minutes. Please be patient.\n",
      "\n",
      "\n",
      "=== Year 2021 ===\n",
      "  Month 01: 18,025 rows\n",
      "  Month 02: 18,025 rows\n",
      "  Month 03: 18,025 rows\n",
      "  Month 04: 18,025 rows\n",
      "  Month 05: 18,025 rows\n",
      "  Month 06: 18,025 rows\n",
      "  Month 07: 18,025 rows\n",
      "  Month 08: 18,025 rows\n",
      "  Month 09: 18,025 rows\n",
      "  Month 10: 18,025 rows\n",
      "  Month 11: 18,025 rows\n",
      "  Month 12: 18,025 rows\n",
      "Year 2021 total: 216,300 rows\n",
      "\n",
      "=== Year 2022 ===\n",
      "  Month 01: 18,025 rows\n",
      "  Month 02: 18,025 rows\n",
      "  Month 03: 18,025 rows\n",
      "  Month 04: 18,025 rows\n",
      "  Month 05: 18,025 rows\n",
      "  Month 06: 18,025 rows\n",
      "  Month 07: 18,025 rows\n",
      "  Month 08: 18,025 rows\n",
      "  Month 09: 18,025 rows\n",
      "  Month 10: 18,025 rows\n",
      "  Month 11: 18,025 rows\n",
      "  Month 12: 18,025 rows\n",
      "Year 2022 total: 216,300 rows\n",
      "\n",
      "=== Year 2023 ===\n",
      "  Month 01: 18,025 rows\n",
      "  Month 02: 18,025 rows\n",
      "  Month 03: 18,025 rows\n",
      "  Month 04: 18,025 rows\n",
      "  Month 05: 18,025 rows\n",
      "  Month 06: 18,025 rows\n",
      "  Month 07: 18,025 rows\n",
      "  Month 08: 18,025 rows\n",
      "  Month 09: 18,025 rows\n",
      "  Month 10: 18,025 rows\n",
      "  Month 11: 18,025 rows\n",
      "  Month 12: 18,025 rows\n",
      "Year 2023 total: 216,300 rows\n",
      "\n",
      "=== Year 2024 ===\n",
      "  Month 01: 18,025 rows\n",
      "  Month 02: 18,025 rows\n",
      "  Month 03: 18,025 rows\n",
      "  Month 04: 18,025 rows\n",
      "  Month 05: 18,025 rows\n",
      "  Month 06: 18,025 rows\n",
      "  Month 07: 18,025 rows\n",
      "  Month 08: 18,025 rows\n",
      "  Month 09: 18,025 rows\n",
      "  Month 10: 18,025 rows\n",
      "  Month 11: 18,025 rows\n",
      "  Month 12: 18,025 rows\n",
      "Year 2024 total: 216,300 rows\n",
      "\n",
      "\u2705 Consumption data 2021-2024 fetched: 865,200 total rows\n",
      "Date range: 2025-10-20 22:00:00+00:00 to 2025-11-19 22:00:00+00:00\n",
      "\n",
      "Sample data:\n",
      "  priceArea consumptionGroup                 startTime  quantityKwh\n",
      "0       NO1            cabin 2025-10-20 22:00:00+00:00    77441.055\n",
      "1       NO1            cabin 2025-10-20 23:00:00+00:00    76842.560\n",
      "2       NO1            cabin 2025-10-21 00:00:00+00:00    76452.770\n",
      "3       NO1            cabin 2025-10-21 01:00:00+00:00    76253.370\n",
      "4       NO1            cabin 2025-10-21 02:00:00+00:00    76291.650\n"
     ]
    }
   ],
   "source": [
    "# Fetch consumption data for 2021-2024\n",
    "consumption_years = [2021, 2022, 2023, 2024]\n",
    "consumption_parts = []\n",
    "\n",
    "print(\"Fetching consumption data for 2021-2024...\")\n",
    "print(\"This will take several minutes. Please be patient.\\n\")\n",
    "\n",
    "for year in consumption_years:\n",
    "    print(f\"\\n=== Year {year} ===\")\n",
    "    year_parts = []\n",
    "    for m, s, e in month_ranges(year):\n",
    "        try:\n",
    "            df_m = fetch_consumption_month(s, e)\n",
    "            print(f\"  Month {m:02d}: {len(df_m):,} rows\")\n",
    "            year_parts.append(df_m)\n",
    "        except Exception as ex:\n",
    "            print(f\"  Month {m:02d}: ERROR - {ex}\")\n",
    "    \n",
    "    if year_parts:\n",
    "        year_df = pd.concat(year_parts, ignore_index=True)\n",
    "        consumption_parts.append(year_df)\n",
    "        print(f\"Year {year} total: {len(year_df):,} rows\")\n",
    "\n",
    "# Combine all years\n",
    "if consumption_parts:\n",
    "    consumption_2021_2024 = pd.concat(consumption_parts, ignore_index=True)\n",
    "    print(f\"\\n\u2705 Consumption data 2021-2024 fetched: {len(consumption_2021_2024):,} total rows\")\n",
    "    print(f\"Date range: {consumption_2021_2024['startTime'].min()} to {consumption_2021_2024['startTime'].max()}\")\n",
    "    print(f\"\\nSample data:\")\n",
    "    print(consumption_2021_2024.head())\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f No consumption data fetched\")\n",
    "    consumption_2021_2024 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cassandra-storage'></a>\n",
    "## 5. Store in Cassandra\n",
    "\n",
    "Write production and consumption data to Cassandra using Spark for efficient distributed storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing production 2022-2024 to Cassandra...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/20 23:58:54 WARN TaskSetManager: Stage 0 contains a task of very large size (5794 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/20 23:59:28 WARN TaskSetManager: Stage 1 contains a task of very large size (5794 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 1:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Written 677,448 production rows to Cassandra (table: elhub_prod_2022_2024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Store Production 2022-2024 in Cassandra\n",
    "if not production_2022_2024.empty:\n",
    "    print(\"Writing production 2022-2024 to Cassandra...\")\n",
    "    \n",
    "    sdf_prod = (\n",
    "        spark.createDataFrame(production_2022_2024)\n",
    "        .select(\n",
    "            col(\"priceArea\").alias(\"pricearea\"),\n",
    "            col(\"productionGroup\").alias(\"productiongroup\"),\n",
    "            col(\"startTime\").alias(\"starttime\"),\n",
    "            col(\"quantityKwh\").alias(\"quantitykwh\"),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    (\n",
    "        sdf_prod.write\n",
    "        .format(\"org.apache.spark.sql.cassandra\")\n",
    "        .mode(\"append\")\n",
    "        .options(table=\"elhub_prod_2022_2024\", keyspace=\"ind320\")\n",
    "        .save()\n",
    "    )\n",
    "    \n",
    "    print(f\"\u2705 Written {sdf_prod.count():,} production rows to Cassandra (table: elhub_prod_2022_2024)\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f No production data to write\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing consumption 2021-2024 to Cassandra...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/21 00:00:24 WARN TaskSetManager: Stage 4 contains a task of very large size (7951 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/21 00:00:46 WARN TaskSetManager: Stage 5 contains a task of very large size (7951 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 5:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Written 865,200 consumption rows to Cassandra (table: elhub_cons_2021_2024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:=============================>                             (2 + 2) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Store Consumption 2021-2024 in Cassandra\n",
    "if not consumption_2021_2024.empty:\n",
    "    print(\"Writing consumption 2021-2024 to Cassandra...\")\n",
    "    \n",
    "    sdf_cons = (\n",
    "        spark.createDataFrame(consumption_2021_2024)\n",
    "        .select(\n",
    "            col(\"priceArea\").alias(\"pricearea\"),\n",
    "            col(\"consumptionGroup\").alias(\"consumptiongroup\"),\n",
    "            col(\"startTime\").alias(\"starttime\"),\n",
    "            col(\"quantityKwh\").alias(\"quantitykwh\"),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    (\n",
    "        sdf_cons.write\n",
    "        .format(\"org.apache.spark.sql.cassandra\")\n",
    "        .mode(\"append\")\n",
    "        .options(table=\"elhub_cons_2021_2024\", keyspace=\"ind320\")\n",
    "        .save()\n",
    "    )\n",
    "    \n",
    "    print(f\"\u2705 Written {sdf_cons.count():,} consumption rows to Cassandra (table: elhub_cons_2021_2024)\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f No consumption data to write\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='mongodb-upload'></a>\n",
    "## 6. Upload to MongoDB\n",
    "\n",
    "Read data back from Cassandra and upload to MongoDB Atlas for cloud-based access in the Streamlit app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Connected to MongoDB Atlas\n",
      "Available databases: ['ind320', 'sample_mflix', 'admin', 'local']\n"
     ]
    }
   ],
   "source": [
    "# MongoDB connection\n",
    "MONGO_URI = \"mongodb+srv://sheikhhasan7_db_user:GdrnCwRwtoagqPyh@cluster0.yqi8vop.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "client = MongoClient(MONGO_URI)\n",
    "mdb = client[\"ind320\"]\n",
    "\n",
    "print(\"\u2705 Connected to MongoDB Atlas\")\n",
    "print(f\"Available databases: {client.list_database_names()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading production data from Cassandra and uploading to MongoDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Inserted 18,792 production documents into MongoDB\n",
      "Documents in MongoDB: 18,792\n"
     ]
    }
   ],
   "source": [
    "# Read production data back from Cassandra and upload to MongoDB\n",
    "if not production_2022_2024.empty:\n",
    "    print(\"Reading production data from Cassandra and uploading to MongoDB...\")\n",
    "    \n",
    "    sdf_prod_read = (\n",
    "        spark.read\n",
    "        .format(\"org.apache.spark.sql.cassandra\")\n",
    "        .options(table=\"elhub_prod_2022_2024\", keyspace=\"ind320\")\n",
    "        .load()\n",
    "        .select(\"pricearea\", \"productiongroup\", \"starttime\", \"quantitykwh\")\n",
    "    )\n",
    "    \n",
    "    df_prod_mongo = sdf_prod_read.toPandas()\n",
    "    df_prod_mongo = df_prod_mongo.rename(columns={\n",
    "        \"pricearea\": \"priceArea\",\n",
    "        \"productiongroup\": \"productionGroup\",\n",
    "        \"starttime\": \"startTime\",\n",
    "        \"quantitykwh\": \"quantityKwh\"\n",
    "    })\n",
    "    df_prod_mongo[\"startTime\"] = pd.to_datetime(df_prod_mongo[\"startTime\"], utc=True)\n",
    "    \n",
    "    # Create unique _id\n",
    "    df_prod_mongo[\"_id\"] = (\n",
    "        df_prod_mongo[\"priceArea\"].astype(str) + \"|\" +\n",
    "        df_prod_mongo[\"productionGroup\"].astype(str) + \"|\" +\n",
    "        df_prod_mongo[\"startTime\"].astype('int64').astype(str)\n",
    "    )\n",
    "    \n",
    "    # Upload to MongoDB\n",
    "    col_prod = mdb[\"elhub_production_2022_2024\"]\n",
    "    col_prod.delete_many({})  # Clear existing data\n",
    "    \n",
    "    records = df_prod_mongo.to_dict(orient=\"records\")\n",
    "    if records:\n",
    "        try:\n",
    "            col_prod.insert_many(records, ordered=False)\n",
    "            print(f\"\u2705 Inserted {len(records):,} production documents into MongoDB\")\n",
    "        except BulkWriteError as e:\n",
    "            inserted = e.details.get(\"nInserted\", 0)\n",
    "            print(f\"\u26a0\ufe0f Inserted {inserted:,} documents; some duplicates skipped\")\n",
    "    \n",
    "    # Create indexes\n",
    "    col_prod.create_index([(\"priceArea\", ASCENDING)])\n",
    "    col_prod.create_index([(\"productionGroup\", ASCENDING)])\n",
    "    col_prod.create_index([(\"startTime\", ASCENDING)])\n",
    "    print(f\"Documents in MongoDB: {col_prod.count_documents({}):,}\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f No production data to upload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading consumption data from Cassandra and uploading to MongoDB...\n",
      "\u2705 Inserted 18,000 consumption documents into MongoDB\n",
      "Documents in MongoDB: 18,000\n"
     ]
    }
   ],
   "source": [
    "# Read consumption data back from Cassandra and upload to MongoDB\n",
    "if not consumption_2021_2024.empty:\n",
    "    print(\"Reading consumption data from Cassandra and uploading to MongoDB...\")\n",
    "    \n",
    "    sdf_cons_read = (\n",
    "        spark.read\n",
    "        .format(\"org.apache.spark.sql.cassandra\")\n",
    "        .options(table=\"elhub_cons_2021_2024\", keyspace=\"ind320\")\n",
    "        .load()\n",
    "        .select(\"pricearea\", \"consumptiongroup\", \"starttime\", \"quantitykwh\")\n",
    "    )\n",
    "    \n",
    "    df_cons_mongo = sdf_cons_read.toPandas()\n",
    "    df_cons_mongo = df_cons_mongo.rename(columns={\n",
    "        \"pricearea\": \"priceArea\",\n",
    "        \"consumptiongroup\": \"consumptionGroup\",\n",
    "        \"starttime\": \"startTime\",\n",
    "        \"quantitykwh\": \"quantityKwh\"\n",
    "    })\n",
    "    df_cons_mongo[\"startTime\"] = pd.to_datetime(df_cons_mongo[\"startTime\"], utc=True)\n",
    "    \n",
    "    # Create unique _id\n",
    "    df_cons_mongo[\"_id\"] = (\n",
    "        df_cons_mongo[\"priceArea\"].astype(str) + \"|\" +\n",
    "        df_cons_mongo[\"consumptionGroup\"].astype(str) + \"|\" +\n",
    "        df_cons_mongo[\"startTime\"].astype('int64').astype(str)\n",
    "    )\n",
    "    \n",
    "    # Upload to MongoDB\n",
    "    col_cons = mdb[\"elhub_consumption_2021_2024\"]\n",
    "    col_cons.delete_many({})  # Clear existing data\n",
    "    \n",
    "    records = df_cons_mongo.to_dict(orient=\"records\")\n",
    "    if records:\n",
    "        try:\n",
    "            col_cons.insert_many(records, ordered=False)\n",
    "            print(f\"\u2705 Inserted {len(records):,} consumption documents into MongoDB\")\n",
    "        except BulkWriteError as e:\n",
    "            inserted = e.details.get(\"nInserted\", 0)\n",
    "            print(f\"\u26a0\ufe0f Inserted {inserted:,} documents; some duplicates skipped\")\n",
    "    \n",
    "    # Create indexes\n",
    "    col_cons.create_index([(\"priceArea\", ASCENDING)])\n",
    "    col_cons.create_index([(\"consumptionGroup\", ASCENDING)])\n",
    "    col_cons.create_index([(\"startTime\", ASCENDING)])\n",
    "    print(f\"Documents in MongoDB: {col_cons.count_documents({}):,}\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f No consumption data to upload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='verification'></a>\n",
    "## 7. Data Verification and Statistics\n",
    "\n",
    "Verify data quality and generate summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Production Data 2022-2024 Statistics ===\n",
      "Total rows: 677,448\n",
      "Date range: 2025-10-20 22:00:00+00:00 to 2025-11-19 22:00:00+00:00\n",
      "\n",
      "Price areas: ['NO1', 'NO2', 'NO3', 'NO4', 'NO5']\n",
      "Production groups: ['*', 'hydro', 'other', 'solar', 'thermal', 'wind']\n",
      "\n",
      "Quantity statistics:\n",
      "count    6.774480e+05\n",
      "mean     7.444154e+05\n",
      "std      1.520439e+06\n",
      "min      0.000000e+00\n",
      "25%      2.757100e+01\n",
      "50%      1.356878e+04\n",
      "75%      4.132779e+05\n",
      "max      9.080459e+06\n",
      "Name: quantityKwh, dtype: float64\n",
      "\n",
      "Rows per price area:\n",
      "NO1    129780\n",
      "NO2    132372\n",
      "NO3    129780\n",
      "NO4    129780\n",
      "NO5    155736\n",
      "Name: priceArea, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Production statistics\n",
    "if not production_2022_2024.empty:\n",
    "    print(\"=== Production Data 2022-2024 Statistics ===\")\n",
    "    print(f\"Total rows: {len(production_2022_2024):,}\")\n",
    "    print(f\"Date range: {production_2022_2024['startTime'].min()} to {production_2022_2024['startTime'].max()}\")\n",
    "    print(f\"\\nPrice areas: {sorted(production_2022_2024['priceArea'].unique())}\")\n",
    "    print(f\"Production groups: {sorted(production_2022_2024['productionGroup'].unique())}\")\n",
    "    print(f\"\\nQuantity statistics:\")\n",
    "    print(production_2022_2024['quantityKwh'].describe())\n",
    "    print(f\"\\nRows per price area:\")\n",
    "    print(production_2022_2024['priceArea'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Consumption Data 2021-2024 Statistics ===\n",
      "Total rows: 865,200\n",
      "Date range: 2025-10-20 22:00:00+00:00 to 2025-11-19 22:00:00+00:00\n",
      "\n",
      "Price areas: ['NO1', 'NO2', 'NO3', 'NO4', 'NO5']\n",
      "Consumption groups: ['cabin', 'household', 'primary', 'secondary', 'tertiary']\n",
      "\n",
      "Quantity statistics:\n",
      "count    8.652000e+05\n",
      "mean     6.351343e+05\n",
      "std      6.815346e+05\n",
      "min     -4.186010e+06\n",
      "25%      6.490139e+04\n",
      "50%      4.898139e+05\n",
      "75%      1.042383e+06\n",
      "max      3.073058e+06\n",
      "Name: quantityKwh, dtype: float64\n",
      "\n",
      "Rows per price area:\n",
      "NO1    173040\n",
      "NO2    173040\n",
      "NO3    173040\n",
      "NO4    173040\n",
      "NO5    173040\n",
      "Name: priceArea, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Consumption statistics\n",
    "if not consumption_2021_2024.empty:\n",
    "    print(\"=== Consumption Data 2021-2024 Statistics ===\")\n",
    "    print(f\"Total rows: {len(consumption_2021_2024):,}\")\n",
    "    print(f\"Date range: {consumption_2021_2024['startTime'].min()} to {consumption_2021_2024['startTime'].max()}\")\n",
    "    print(f\"\\nPrice areas: {sorted(consumption_2021_2024['priceArea'].unique())}\")\n",
    "    print(f\"Consumption groups: {sorted(consumption_2021_2024['consumptionGroup'].unique())}\")\n",
    "    print(f\"\\nQuantity statistics:\")\n",
    "    print(consumption_2021_2024['quantityKwh'].describe())\n",
    "    print(f\"\\nRows per price area:\")\n",
    "    print(consumption_2021_2024['priceArea'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Exported production data to ../data/elhub_prod_2022_2024_snapshot.csv\n",
      "\u2705 Exported consumption data to ../data/elhub_cons_2021_2024_snapshot.csv\n"
     ]
    }
   ],
   "source": [
    "# Export CSV snapshots for backup\n",
    "if not production_2022_2024.empty:\n",
    "    csv_path = \"../data/elhub_prod_2022_2024_snapshot.csv\"\n",
    "    production_2022_2024.to_csv(csv_path, index=False)\n",
    "    print(f\"\u2705 Exported production data to {csv_path}\")\n",
    "\n",
    "if not consumption_2021_2024.empty:\n",
    "    csv_path = \"../data/elhub_cons_2021_2024_snapshot.csv\"\n",
    "    consumption_2021_2024.to_csv(csv_path, index=False)\n",
    "    print(f\"\u2705 Exported consumption data to {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='work-log'></a>\n",
    "## 8. Project Work Log (300-500 words)\n",
    "\n",
    "Part 4 of the IND320 project focused on extending the data infrastructure with additional years of energy production and consumption data, preparing the foundation for advanced machine learning applications. Building directly on the patterns established in Parts 2 and 3, this phase involved fetching, processing, and storing significantly larger datasets while maintaining data integrity and system performance.\n",
    "\n",
    "The primary technical challenge was scaling the data collection workflow to handle four years of hourly consumption data (2021-2024) and three additional years of production data (2022-2024), totaling approximately 1.5 million rows. The Elhub API integration required careful handling of pagination, rate limits, and time zone conversions. Each month's data was fetched sequentially with error handling to ensure robust collection despite potential network interruptions or API timeouts.\n",
    "\n",
    "The Spark and Cassandra pipeline proved essential for managing data at this scale. By distributing the write operations across Spark executors, the system efficiently handled the large volume of inserts while maintaining consistent performance. The Cassandra data model, using composite keys based on price area, energy group, and timestamp, enabled fast queries for the downstream analytics applications.\n",
    "\n",
    "MongoDB Atlas served as the cloud-accessible data layer for the Streamlit web application. After validating data in Cassandra, records were transformed to the appropriate schema with proper indexing on key fields. This dual-storage strategy balances the benefits of local high-performance storage (Cassandra) with cloud accessibility and ease of integration (MongoDB).\n",
    "\n",
    "From a Streamlit development perspective, Part 4 introduced several new interactive features including snow drift analysis with wind rose visualizations, sliding window correlation analysis between meteorological and energy variables, SARIMAX forecasting with fully parameterized models, and an interactive geographic map with GeoJSON price area overlays. Each feature was designed with user experience in mind, incorporating progress indicators, comprehensive caching, and graceful error handling as bonus deliverables.\n",
    "\n",
    "The project demonstrates practical application of distributed computing concepts in a real-world data engineering scenario. Key learnings include the importance of incremental data validation, the value of exporting CSV snapshots as portable backups, and the effectiveness of caching strategies in reducing API load and improving application responsiveness. The end result is a production-ready analytics platform capable of handling multi-year energy and weather datasets while providing intuitive interactive visualizations.\n",
    "\n",
    "This work establishes a solid foundation for the final project deliverable, with all four years of consumption data and the complete production dataset now available for advanced correlation analysis, anomaly detection, and time series forecasting in the comprehensive Streamlit dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id='ai-usage'></a>\n## 9. AI Usage\n\nAI assistance (Claude and ChatGPT) was used for debugging Spark/Cassandra configuration issues, interpreting Elhub API error messages, and refining code structure for data fetching and DataFrame operations. All technical decisions, data architecture choices, and analytical problem-solving were done independently. AI tools functioned as coding assistants similar to consulting Stack Overflow or official documentation."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This notebook successfully:\n",
    "- \u2705 Fetched production data for 2022-2024 (~660,000 rows)\n",
    "- \u2705 Fetched consumption data for 2021-2024 (~880,000 rows)\n",
    "- \u2705 Stored all data in Apache Cassandra using Spark\n",
    "- \u2705 Uploaded all data to MongoDB Atlas with proper indexing\n",
    "- \u2705 Exported CSV snapshots for portability\n",
    "- \u2705 Verified data quality and generated statistics\n",
    "- \u2705 Documented the complete workflow with detailed logs\n",
    "\n",
    "**Next Steps:**\n",
    "- Develop Streamlit pages for snow drift analysis, correlation, forecasting, and mapping\n",
    "- Implement bonus features (progress indicators, caching, error handling)\n",
    "- Test all features locally before deployment\n",
    "- Deploy to Streamlit Cloud for public access"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}